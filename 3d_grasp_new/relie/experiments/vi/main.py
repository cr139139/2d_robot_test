"""
Simple example of Variational Inference.

We have known data X_0 (in a Linear G space).
We observe some single X that is generated by G_0(X_0) by unknown G_0.
We have given likelihood model:
p(X|G) = N(X | G(X_0), sigma)

Now we do VI to infer p(G|X), so we optimise the ELBO:
for variational family q(G) and uniform prior:

E_{G \sim q) \log p(X|G) - log q(G)


A couple of geometries are studied.
Variational distribution is either flow (restricted to <pi) or Gaussian.

Loss is symmetry invariant loss function.
"""
import math
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import numpy as np
import torch
import torch.nn as nn

from relie.utils.experiment import print_log_summary
from relie.experiments.vi.flow_distribution import Flow, FlowDistribution
from relie.experiments.vi.pushed_gaussian_distribution import PushedGaussianDistribution
from relie.lie_distr import SO3Prior
from relie.utils.so3_tools import so3_log, so3_vee
from relie.utils.geometry import (
    cyclic_coordinates,
    invariant_loss,
    cyclic_permutations,
    rotation_matrices,
)

from relie.utils.metropolis_hastings import so3_mh

torch.manual_seed(0)

# Random pointcloud
# x_zero = torch.randn(100, 3, dtype=torch.double)
# symmetry = None

# Two points
x_zero = torch.tensor(cyclic_coordinates(2)).double()
symmetry = np.eye(3)[None, ...]
# symmetry = rotation_matrices(cyclic_coordinates(2), cyclic_permutations(2))

# Equilateral triangle
x_zero = torch.tensor(cyclic_coordinates(3)).double()
symmetry = rotation_matrices(cyclic_coordinates(3), cyclic_permutations(3))

# Tetrahedron
# x_zero = torch.tensor(tetrahedron_coordinates()).double()
# symmetry = rotation_matrices(tetrahedron_coordinates(), tetrahedron_permutations())


# Representation
# x_zero = block_wigner_matrix_multiply(
#     so3_matrix_to_eazyz(symmetry).float(),
#     x_zero.expand(len(symmetry), -1, -1), 3)
# x_zero = x_zero.mean(0)

g_zero = SO3Prior(dtype=torch.double).sample((1,))[0]
# g_zero = torch.eye(3, dtype=torch.double)
g_zero_alg = so3_vee(so3_log(g_zero))
x = (g_zero @ x_zero.t()).t()


symmetry = torch.tensor(symmetry).double()


def prediction_loss_fn(g, x, x_zero):
    """
    Prediction loss = -log p(X|G)
    Allows for batching in G.
    :param g: Group shape (b, 3, 3)
    :param x: (n, 3)
    :param x_zero: (n, 3)
    :return: (b)
    """
    sym = torch.eye(3)[None].double() if symmetry is None else symmetry
    y = torch.einsum("bij,nj->bni", [g, x_zero])  # [b, n, 3]
    x = x.expand_as(y)
    l = invariant_loss(x.contiguous(), y.contiguous(), sym)
    return l.mean(1)


class VIModel(nn.Module):
    def __init__(self, distr):
        super().__init__()
        self.distr = distr
        self.beta = 1e-1

    def forward(self, x, x_zero):
        distr = self.distr()
        g = distr.rsample((64,))

        prediction_loss = prediction_loss_fn(g, x, x_zero).float()
        entropy = -distr.log_prob(g)
        loss = prediction_loss - entropy * self.beta
        return (
            loss,
            {
                "loss": loss.mean().item(),
                "prediction": prediction_loss.mean().item(),
                "entropy": entropy.mean().item(),
            },
        )


def plot_group_samples(model, true_post=None):
    model.eval()
    num_noise_samples = 1000
    true_post_ = true_post.view(-1, 9)
    inferred_distr = model.distr()
    inferred_samples = inferred_distr.sample((num_noise_samples,)).view(-1, 9)

    if true_post is not None:
        samples = torch.cat([inferred_samples, true_post_], 0)
    else:
        samples = inferred_samples

    pca = PCA(3).fit(samples)

    fig = plt.figure()
    ax = fig.add_subplot(111, projection="3d")
    ax.scatter(
        *pca.transform(inferred_samples).T, label="Model samples", alpha=0.1, color="r"
    )
    ax.scatter(
        *pca.transform(true_post_).T, label="Model samples", alpha=0.1, color="b"
    )
    ax.view_init(70, 30)
    # plt.legend()
    plt.tight_layout()
    plt.show()


def plot_samples(samples):

    samples_ = samples.view(-1, 9)
    pca = PCA(3).fit(samples_)

    fig = plt.figure()
    ax = fig.add_subplot(111, projection="3d")
    ax.scatter(*pca.transform(samples_).T, label="Model samples", alpha=0.1)
    ax.view_init(70, 30)
    # proj_samples = pca.transform(samples_)
    # print(proj_samples.shape)
    # mask = (proj_samples[:,0]**2<0.1)+(proj_samples[:,1]**2<0.1)+(proj_samples[:,2]**2<0.1)
    # print(samples.numpy()[mask])
    # plt.legend()
    plt.tight_layout()
    plt.show()


# running MH
def log_energy(g):
    return -2 * prediction_loss_fn(g, x, x_zero)


true_post = so3_mh(log_energy, 1000, 1000)[-1]

plot_samples(true_post)

mask = log_energy(true_post) > -0.5

plot_samples(true_post[mask])
# print(log_energy(true_post[mask]))


flow = Flow(3, 12, batch_norm=False)
flow_distr = FlowDistribution(flow, math.pi * 1.0)
gaussian_distr = PushedGaussianDistribution(lie_multiply=True)
# model = VIModel(gaussian_distr)
model = VIModel(flow_distr)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

infos = []
for it in range(50000):
    model.train()
    # with torch.autograd.detect_anomaly():
    loss, info = model(x, x_zero)
    optimizer.zero_grad()
    loss.mean().backward()
    optimizer.step()
    infos.append(info)

    for n, p in model.named_parameters():
        assert torch.isnan(p).sum() == 0, f"NaN in parameters {n}"

    if it % 1000 == 0:
        print_log_summary(it, 1000, infos)
        plot_group_samples(model, true_post)
        if model.distr is gaussian_distr:
            print(
                f"Parameters: {torch.cat([gaussian_distr.loc, gaussian_distr.scale]).tolist()}"
            )
            print(f"Target at {g_zero_alg.tolist()}")
